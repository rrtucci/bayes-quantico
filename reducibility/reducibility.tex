\chapter{Reducibility of Representations}
\label{ch-reducibility}

This chapter is based on
Ref.\cite{birdtracks-book}.

\section{Eigenvalue Projectors}

Suppose $M\in \CC^{d\times d }$ 
has eigenvalues 
$\lam_i$ with corresponding eigenvectors $\ket{\lam_i}$

\beq
M\ket{\lam_i}=\lam_i \ket{\lam_i}
\eeq
for $i\in \ZZ_{[1,r]}$.
The characteristic polynomial of $M$ is
defined as

\beq
cp(\lam)\eqdef \det(M-\lam)= \prod_{i=1}^r (\lam-\lam_i)^{d_i}
\eeq
It must satisfy

\beq
cp(\lam) =0
\eeq

Note 
that if $M$ is Hermitian 
($M^\dagger=M$),
then all its eigenvalues  are real. (because $\lam_i =
\av{\lam_i|M|\lam_i}\in\RR$)




If $M$ is a Hermitian, then there exists
a matrix $C$ that is a unitary ($CC^\dagger = C^\dagger C =1$)
and diagonalizes $M$

\beq
CMC^\dagger=
\left[
\begin{array}{cccc}
D_{\lam_1}
&0
&0
&0
\\
0
&D_{\lam_2}
&0
&0
\\
0
&0
&\ddots
&0
\\
0
&0
&0
&D_{\lam_r}
\end{array}
\right]
\eeq
where

\beq
D_{\lam_i} =
\text{diag}\underbrace{(\lam_i,\lam_i, \dots,\lam_i)}_{d_i\text{ times}}
\eeq

\beq
d=\sum_{i=1}^r d_i
\eeq
For example,
when $d=2$, 


\beq
CMC^\dagger =
\left[
\begin{array}{cc}
\lam_1 &0
\\
0&\lam_2
\end{array}
\right]
\eeq
Note that for $d=2$,

\beq
C P_1 C^\dagger=
\left[
\begin{array}{cc}
1&0
\\
0&0
\end{array}
\right]
=
\frac{CMC^\dagger -\lam_2}{\lam_1-\lam_2}
\eeq

\beq
CP_2 C^\dagger =
\left[
\begin{array}{cc}
0&0
\\
0&1
\end{array}
\right]
=
\frac{CMC^\dagger-\lam_1}{\lam_2-\lam_1}
\eeq
$P_1$ and $P_2$ are a 
set of complete
orthogonal projection operators

\beq
P_1 + P_2 =1
\eeq

\beq
P_1^2=P_1,\; P_2^2=P_2,\; P_1P_2=P_2P_1=0
\eeq

Similarly, for $d>2$, we can 
define
one projection
operator $P_i$ 
for each eigenvalue
 $\lam_i$.
If $I^{d_i\times d_i}$
is the $d_i$
dimensional unit matrix,
then
\beqa
P_i &=&
C^\dagger
diag(0,\ldots,0, I^{d_i\times d_i},0, \dots, 0)C
\\
&=&
\prod_{j\neq i}
\frac{M -\lam_j}{\lam_i -\lam_j}
\eeqa
As for $d=2$, the $P_i$
just defined are 
a complete set of 
orthogonal projection operators:

\beq
\sum_{i=1}^r P_i=1
\quad\text{(completeness)}
\eeq

\beq
P_iP_j =P_i\delta(i,j)
\quad \text{(orthonormality)}
\eeq
for all $i,j\in \ZZ_{[1,r]}$

Note that
\beqa
d_i &=&\tr [C^\dagger P_i C]
\\
&=& \tr P_i
\eeqa

Note that the $P_i$'s are Hermitian
($P_i^\dagger = P_i$)
because $M$
is Hermitian and
its eigenvalues are real.

\section{$[P_i, M]=0$ consequences}

Note that for any $i$,
$P_i$ and $M$
commute

\beq
[P_i, M]=
P_iM-MP_i=0
\eeq
From the $P_i$'s completeness and
commutativity with $M$, we get

\beqa
M&=& \sum_{i=1}^r\sum_{j=1}^r
P_iM P_j
\\
&=&
\sum_{i=1}^r
P_iM P_i
\eeqa

\begin{claim}
For all $i$,
\beq
MP_i = \lam_i P_i \;
\text{(no $i$ sum)}
\eeq
\end{claim}
\proof
We only show it for $d=2$

\beqa
CMP_1C^\dagger &=&
\left[
\begin{array}{cc}
\lam_1&0
\\
0&\lam_2
\end{array}
\right] 
\left[
\begin{array}{cc}
1&0
\\
0&0
\end{array}
\right] 
\\
&=&
\lam_1
\left[
\begin{array}{cc}
1&0
\\
0&0
\end{array}
\right] 
\\
&=&  \lam_i C P_i C^\dagger
\eeqa
\qed


From the 
last claim, it immediately follows that if  $f(x)$ can be
expressed as a 
power series in
$x$, then
\footnote{$M$ must also satisfy
some
convergence conditions
that we won't get into.}

\beq
f(M) P_i = f(\lam_i)P_i \;
\text{(no $i$ sum)}
\eeq

Suppose 
$M^{(1)}, M^{(2)}\in \CC^{d\times d}$
are Hermitian matrices that
commute

\beq
[M^{(1)}, M^{(2)}]  =0
\eeq
Use $M^{(1)}$ to decompose $V=\CC^{d\times d}$
into 
a direct sum of vector spaces $\bigoplus_i V_i$.
Then we can use  $M^{(2)}$ to decompose $V_i$ into
$\bigoplus_j V_{i,j}$. 
If $M^{(1)}$ and $M^{(2)}$ don't
commute, let $P^{(1)}_i$ be an eigenvalue 
projection operator of $M^{(1)}$. Then replace $M^{(2)}$ by $P^{(1)}_i M^{(2)}P_i^{(1)}$. Now

\beq
[M^{(1)}, P^{(1)}_iM^{(2)}P^{(1)}_i]  =0
\eeq

\section{$[G, M]=0$  consequences}

An invariant matrix (see Ch.\ref{ch-invariants}) commutes with 
all the elements $G$ of a group $\calg$

\beq
[G, M] =0
\eeq
If $P_i$ are 
the projection operators of $M$, then $P_i=f_i(M)$ so

\beq
[G, P_i]=0
\eeq
for all $G\in \calg$ and $i$.


\beq
G = 1G1 =\sum_i\sum_j P_i G P_j
=
\sum_j \underbrace{P_j G P_j}_
{\eqdef G_j}
\eeq

\begin{claim}
\beq
 G = C^\dagger
diag(G_1, G_2, \ldots)C
\eeq

\beq
G=
\sum_i
C^\dagger_i
G_i C_i 
\eeq
where the matrices $C_i$
are the Clebsch Gordan 
coefficients of $M$ (see Ch. \ref{ch-clebsch-gordan})
\end{claim}
\proof

\beq
C_i G C^\dagger_i = 
\sum_j 
C_i P_jG P_j C_i^\dagger
=C_i G_i C_i^\dagger
= G_i
\eeq
\qed


A rep-matrix $G_i$ acts only
on a $d_i$ dimensional vector space $V^{d_i}=P_i V^d$.
In this way, an invariant
matrix $M\in \CC^{d\times d}$
with $r$ 
distinct eigenvalues,
induces a decomposition of $V^d$
into a direct sum of vector spaces

\beq
V^d\xymatrix{\ar[r]_M&}
V_1^{d_1}
\oplus 
V_2^{d_2}
\oplus
\ldots
\oplus 
V_r^{d_r}
\eeq
If a rep-matrix $G_i$ cannot itself be
reduced further, it is said to 
be an {\bf irreducible representation (irrep)}.

Note that sometimes the term representation
is used to refer to the 
vector space $V_i^{d_i}$
instead of the matrix $G_i$.

We've considered the 
decomposition of $V^d$ into irreps. An example of such a decomposition is the decomposition of $V^n\otimes\dual{V }^n$

\beq
\begin{array}{l}
\myboxed{
1 = \frac{1}{n} \twoarrows + P_{Adj} +
\sum_{\lam \neq Adj}P_\lam,
}
\\
\myboxed{
\delta^a_d
\delta_d^c
=
\frac{1}{n}
\delta^a_b
\delta^c_d
+
(P_{Adj})\indices{_a^b_c^d
}
+\sum_{\lam \neq Adj}
(P_{\lam})\indices{_a^b_c^d
}
}
\\
\bcen
\xymatrix@R=1pc@C=1pc{
a
&d\ar[l]
\\
b
&c\ar@{<-}[l]
}
\ecen
=
\frac{1}{n}
\bcen
\xymatrix@R=1pc@C=1pc{
&\ar[dd]
\\
\\
\ar[uu]
&
}
\ecen
+
\bcen
\xymatrix@R=1pc@C=2pc{
&\ar[dd]
\\
&\ar@{~}[l]
\\
\ar[uu]
&
}
\ecen
+\sum_{\lam
\neq Adj}
\bcen
\xymatrix@R=1pc@C=2pc{
&\ar[dd]
\\
&\ar@2{->}[l]^\lam
\\
\ar[uu]
&
}
\ecen
\end{array}
\eeq